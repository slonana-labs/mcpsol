# Research: Minimize Framework Overhead

**Feature**: [spec.md](./spec.md) | **Plan**: [plan.md](./plan.md)

## Research Questions

### RQ1: Safe Unsafe Patterns for BPF

**Question**: What unsafe patterns are sound for discriminator/argument extraction after bounds checking?

**Decision**: Use `ptr::read_unaligned` with explicit safety comments.

**Rationale**:
- BPF environment has no alignment requirements for byte reads
- `ptr::read_unaligned` is safer than raw pointer casts for potentially unaligned data
- Safety invariant: length check performed before any unsafe read

**Pattern**:
```rust
// SAFETY: Length validated at function entry, offset < validated_len
let value = unsafe {
    core::ptr::read_unaligned(data.as_ptr().add(offset) as *const T)
};
```

**Alternatives Considered**:
- Raw pointer cast `*(ptr as *const T)` - Rejected: alignment issues possible
- `slice::get_unchecked` - Acceptable but still requires array conversion
- Keep safe code - Rejected: 70+ CU overhead per argument unacceptable

### RQ2: Compile-Time Offset Calculation

**Question**: Can we calculate argument offsets at compile time instead of runtime tracking?

**Decision**: Yes, use const generics and macro-generated constants.

**Rationale**:
- Argument types have known sizes at compile time
- Offset calculation is pure arithmetic on constants
- Eliminates mutable `__offset` variable entirely

**Pattern**:
```rust
// Generated by macro:
const DISCRIMINATOR_SIZE: usize = 8;
const ARG0_OFFSET: usize = DISCRIMINATOR_SIZE;          // 8
const ARG0_SIZE: usize = 8;                              // u64
const ARG1_OFFSET: usize = ARG0_OFFSET + ARG0_SIZE;     // 16
const EXPECTED_LEN: usize = ARG1_OFFSET + ARG1_SIZE;    // total

// Single check:
if instruction_data.len() < EXPECTED_LEN {
    return Err(InvalidInstructionData);
}

// Direct reads at known offsets:
let arg0 = read_u64_at(instruction_data, ARG0_OFFSET);
let arg1 = read_u64_at(instruction_data, ARG1_OFFSET);
```

**Alternatives Considered**:
- Runtime offset accumulation - Rejected: current approach, ~10 CU overhead
- Struct overlay with `bytemuck` - Considered: would work but adds dependency complexity

### RQ3: Zero-Cost Context Abstraction

**Question**: How can Context be made optional without breaking existing code?

**Decision**: Provide two code paths via attribute flag.

**Rationale**:
- Some instructions need Context (account validation, remaining_accounts)
- Simple instructions just need raw accounts slice
- Forcing Context on all instructions wastes ~50 CU

**Pattern**:
```rust
// Option 1: With Context (existing behavior)
#[mcp_instruction(name = "complex_op", context = true)]
pub fn complex_op(ctx: Context<ComplexAccounts>, amount: u64) -> Result<()>

// Option 2: Without Context (new optimized path)
#[mcp_instruction(name = "simple_op")]
pub fn simple_op(accounts: &[AccountInfo], amount: u64) -> Result<()>
```

**Implementation**:
- Default: no Context (zero overhead)
- Opt-in: `context = true` attribute builds Context
- Backwards compatibility: existing code with Context types still works

**Alternatives Considered**:
- Always build Context - Rejected: wastes CU for simple instructions
- Remove Context entirely - Rejected: breaks existing code, loses useful abstraction

### RQ4: Discriminator Match Optimization

**Question**: Is the current match statement optimal for N discriminators?

**Decision**: Current approach is acceptable for typical N (<20 instructions).

**Rationale**:
- LLVM optimizes small match statements into efficient jump tables or binary search
- For N < 20, linear comparison is ~30 CU total
- Optimization only matters for very large N (>50 instructions)

**Pattern** (current, acceptable):
```rust
match discriminator {
    [0x42, 0x19, ...] => { /* list_tools */ }
    [0xaf, 0xaf, ...] => { /* increment */ }
    _ => Err(InvalidInstructionData)
}
```

**Future Optimization** (if needed for large N):
- Perfect hash function for O(1) dispatch
- Only implement if programs with 50+ instructions become common

**Alternatives Considered**:
- HashMap lookup - Rejected: heap allocation, worse for small N
- Binary search - Rejected: LLVM already does this for medium N
- Perfect hash - Deferred: complexity not justified for typical programs

### RQ5: Error Handling Overhead

**Question**: What's the CU cost of error construction and can it be reduced?

**Decision**: Use lightweight error variants, avoid string formatting.

**Rationale**:
- `ProgramError::InvalidInstructionData` is a simple enum variant (~5 CU)
- Custom error messages with formatting are expensive (~50+ CU)
- Framework errors should be simple; detailed errors are user's responsibility

**Pattern**:
```rust
// Good: Simple enum return
return Err(ProgramError::InvalidInstructionData);

// Bad: String formatting
return Err(ProgramError::Custom(format!("expected {} bytes", n)));
```

**Alternatives Considered**:
- Detailed error messages - Rejected: CU cost too high for framework
- Error codes with lookup table - Deferred: adds complexity, marginal benefit

## Benchmark Baseline

Current overhead measured via `core/tests/cu_benchmarks.rs`:

| Component | Current CU | Target CU |
|-----------|------------|-----------|
| Length check | 20 | 10 |
| Discriminator extraction | 50 | 10 |
| Argument parsing (u64) | 70 | 5 |
| Context building | 50 | 0 (optional) |
| Match dispatch | 30 | 25 |
| **Total** | **220** | **50** |

## Safety Documentation Requirements

All unsafe code must include:

1. `// SAFETY:` comment explaining invariant
2. Reference to the bounds check that establishes safety
3. Assertion in debug builds (compiled out in release)

Example:
```rust
// SAFETY: instruction_data.len() >= EXPECTED_LEN checked at line N
// Debug assertion included for development verification
debug_assert!(offset + 8 <= instruction_data.len());
let value = unsafe {
    core::ptr::read_unaligned(instruction_data.as_ptr().add(offset) as *const u64)
};
```

## Conclusions

1. **Unsafe reads are sound** when preceded by comprehensive length validation
2. **Compile-time offsets** eliminate runtime overhead entirely
3. **Optional Context** preserves backwards compatibility while enabling optimization
4. **Current match dispatch** is acceptable; optimize only if large N becomes common
5. **Simple errors** are sufficient; detailed messages are user responsibility
